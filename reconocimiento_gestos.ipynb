{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presiona 's' para capturar el gesto, 'n' para cambiar de gesto, y 'q' para salir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jufeg\\Documents\\VSC_codes_IA\\venv\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "## CODIGO PARA GUARDAR DATOS DE POSICION DE LOS LANDMARKS EN UN CSV\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "\n",
    "# Inicializar MediaPipe para la detección de manos\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Crear o abrir el archivo CSV\n",
    "with open('gestures_data.csv', mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Escribir encabezado si es un nuevo archivo (21 puntos x, y, y una columna para el gesto)\n",
    "    writer.writerow([f'x{i}' for i in range(21)] + [f'y{i}' for i in range(21)] + ['gesture'])\n",
    "\n",
    "    # Capturar video desde la cámara\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    gesture = input(\"Escribe el nombre del primer gesto a capturar: \")  # Solicitar el primer nombre del gesto\n",
    "    print(\"Presiona 's' para capturar el gesto, 'n' para cambiar de gesto, y 'q' para salir\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Voltear la imagen para un efecto espejo y procesar\n",
    "        image = cv2.flip(image, 1)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        # Verificar si se detectan manos\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Dibujar los landmarks en la imagen\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                # Extraer coordenadas de los 21 landmarks\n",
    "                x_coords = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "                y_coords = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "                \n",
    "                # Mostrar la imagen procesada\n",
    "                cv2.imshow(\"Hand Gesture\", image)\n",
    "\n",
    "                # Teclas para guardar datos, cambiar de gesto, o salir\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('s'):\n",
    "                    # Guardar las coordenadas en el CSV junto con el nombre del gesto\n",
    "                    writer.writerow(x_coords + y_coords + [gesture])\n",
    "                    print(f\"Gesto '{gesture}' capturado y guardado en el CSV.\")\n",
    "                elif key == ord('n'):\n",
    "                    # Cambiar de gesto\n",
    "                    gesture = input(\"Escribe el nuevo nombre del gesto: \")\n",
    "                elif key == ord('q'):\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron valores no numéricos en las columnas de puntos. Revisar el CSV.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jufeg\\Documents\\VSC_codes_IA\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5680 - loss: 1.1160 - val_accuracy: 0.9003 - val_loss: 0.3306\n",
      "Epoch 2/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9219 - loss: 0.2658 - val_accuracy: 0.9934 - val_loss: 0.0821\n",
      "Epoch 3/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9961 - loss: 0.0679 - val_accuracy: 0.9934 - val_loss: 0.0402\n",
      "Epoch 4/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9934 - loss: 0.0394 - val_accuracy: 0.9801 - val_loss: 0.0340\n",
      "Epoch 5/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9886 - loss: 0.0416 - val_accuracy: 0.9934 - val_loss: 0.0200\n",
      "Epoch 6/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9949 - loss: 0.0262 - val_accuracy: 0.9834 - val_loss: 0.0248\n",
      "Epoch 7/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9933 - loss: 0.0183 - val_accuracy: 0.9934 - val_loss: 0.0135\n",
      "Epoch 8/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9998 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.0077\n",
      "Epoch 9/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9929 - loss: 0.0159 - val_accuracy: 0.9834 - val_loss: 0.0268\n",
      "Epoch 10/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0033\n",
      "Epoch 11/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9999 - loss: 0.0040 - val_accuracy: 0.9934 - val_loss: 0.0175\n",
      "Epoch 12/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9999 - loss: 0.0050 - val_accuracy: 0.9934 - val_loss: 0.0110\n",
      "Epoch 13/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9955 - loss: 0.0088 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
      "Epoch 14/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9996 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
      "Epoch 15/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 16/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9996 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 0.0040\n",
      "Epoch 17/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9947 - loss: 0.0101 - val_accuracy: 1.0000 - val_loss: 0.0019\n",
      "Epoch 18/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 7.7401e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 7.5096e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 6.8904e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 9.8883e-04 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 22/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 7.2774e-04 - val_accuracy: 1.0000 - val_loss: 4.8635e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 9.8163e-04 - val_accuracy: 1.0000 - val_loss: 7.8214e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9989 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 3.8772e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 6.5957e-04 - val_accuracy: 1.0000 - val_loss: 4.7891e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9995 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 2.4024e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 1.9273e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.3254e-04 - val_accuracy: 1.0000 - val_loss: 2.7457e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9994 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9971 - loss: 0.0096 - val_accuracy: 0.9269 - val_loss: 0.2789\n",
      "Epoch 31/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9788 - loss: 0.0891 - val_accuracy: 1.0000 - val_loss: 5.5981e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 6.6732e-04 - val_accuracy: 1.0000 - val_loss: 2.9061e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.6051e-04 - val_accuracy: 1.0000 - val_loss: 2.0309e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.2853e-04 - val_accuracy: 1.0000 - val_loss: 1.3470e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.6351e-04 - val_accuracy: 1.0000 - val_loss: 1.1322e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.8201e-04 - val_accuracy: 1.0000 - val_loss: 8.6792e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.3123e-04 - val_accuracy: 1.0000 - val_loss: 8.4400e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.1747e-04 - val_accuracy: 1.0000 - val_loss: 6.2304e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 2.1765e-04 - val_accuracy: 1.0000 - val_loss: 5.4437e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 8.2606e-05 - val_accuracy: 1.0000 - val_loss: 9.6777e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.6535e-04 - val_accuracy: 1.0000 - val_loss: 4.7458e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.2277e-04 - val_accuracy: 1.0000 - val_loss: 4.4291e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 9.0813e-05 - val_accuracy: 1.0000 - val_loss: 3.7738e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.3147e-05 - val_accuracy: 1.0000 - val_loss: 3.8291e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 9.8971e-05 - val_accuracy: 1.0000 - val_loss: 3.0934e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 8.5689e-05 - val_accuracy: 1.0000 - val_loss: 2.8352e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 3.7257e-05 - val_accuracy: 1.0000 - val_loss: 3.7566e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 7.0428e-05 - val_accuracy: 1.0000 - val_loss: 3.3509e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 8.1744e-05 - val_accuracy: 1.0000 - val_loss: 3.5337e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 6.1531e-05 - val_accuracy: 1.0000 - val_loss: 2.3170e-05\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 4.3335e-05 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el conjunto de prueba: 1.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CODIGO DE CREACION DEL MODELO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Cargar los datos del CSV\n",
    "data =pd.read_csv('gestures_data.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Convertir todas las columnas de X a valores numéricos\n",
    "for column in data.columns[:-1]:  # Excluir la última columna que es 'gesture'\n",
    "    data[column] = pd.to_numeric(data[column], errors='coerce')\n",
    "\n",
    "# Verificar si hay valores NaN después de la conversión\n",
    "if data.isnull().values.any():\n",
    "    print(\"Se encontraron valores no numéricos en las columnas de puntos. Revisar el CSV.\")\n",
    "    data = data.dropna()  # Eliminar filas con valores NaN\n",
    "\n",
    "# Separar características (X) y etiquetas (y)\n",
    "X = data.drop('gesture', axis=1).values  # Todas las columnas menos 'gesture'\n",
    "y = data['gesture'].values               # Solo la columna 'gesture'\n",
    "\n",
    "# Codificar las etiquetas de texto a números\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y = to_categorical(y)  # Convertir a formato categórico para clasificación\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construir el modelo de red neuronal\n",
    "model = Sequential([\n",
    "    Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(y.shape[1], activation='softmax')  # Última capa para clasificación con softmax\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Precisión en el conjunto de prueba: {accuracy:.2f}\")\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save('gesture_recognition_model.h5')\n",
    "\n",
    "# Guardar el codificador de etiquetas para usar en predicciones\n",
    "import joblib\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "SerialException",
     "evalue": "could not open port 'COM5': FileNotFoundError(2, 'El sistema no puede encontrar el archivo especificado.', None, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSerialException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m serial_port \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOM5\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Ajusta al puerto COM donde esté el Arduino\u001b[39;00m\n\u001b[0;32m     28\u001b[0m baud_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9600\u001b[39m\n\u001b[1;32m---> 29\u001b[0m ser \u001b[38;5;241m=\u001b[39m \u001b[43mserial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserial_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaud_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Tiempo para que se establezca la conexión\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Inicializar la cámara\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jufeg\\Documents\\VSC_codes_IA\\venv\\lib\\site-packages\\serial\\serialwin32.py:33\u001b[0m, in \u001b[0;36mSerial.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlapped_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlapped_write \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28msuper\u001b[39m(Serial, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jufeg\\Documents\\VSC_codes_IA\\venv\\lib\\site-packages\\serial\\serialutil.py:244\u001b[0m, in \u001b[0;36mSerialBase.__init__\u001b[1;34m(self, port, baudrate, bytesize, parity, stopbits, timeout, xonxoff, rtscts, write_timeout, dsrdtr, inter_byte_timeout, exclusive, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munexpected keyword arguments: \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(kwargs))\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m port \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jufeg\\Documents\\VSC_codes_IA\\venv\\lib\\site-packages\\serial\\serialwin32.py:64\u001b[0m, in \u001b[0;36mSerial.open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port_handle \u001b[38;5;241m==\u001b[39m win32\u001b[38;5;241m.\u001b[39mINVALID_HANDLE_VALUE:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_port_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m    \u001b[38;5;66;03m# 'cause __del__ is called anyway\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SerialException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not open port \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mportstr, ctypes\u001b[38;5;241m.\u001b[39mWinError()))\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overlapped_read \u001b[38;5;241m=\u001b[39m win32\u001b[38;5;241m.\u001b[39mOVERLAPPED()\n",
      "\u001b[1;31mSerialException\u001b[0m: could not open port 'COM5': FileNotFoundError(2, 'El sistema no puede encontrar el archivo especificado.', None, 2)"
     ]
    }
   ],
   "source": [
    "## IMPLEMENTACION DEL MMODELO PARA RECONOCIMIENTO DE GESTOS Y CONEXION SERIAL CON ARDUINO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "import serial\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suprimir advertencias de TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Cargar el modelo entrenado y el codificador de etiquetas\n",
    "model = tf.keras.models.load_model('gesture_recognition_model.h5')\n",
    "label_encoder = joblib.load('label_encoder.pkl')\n",
    "\n",
    "# Inicializar Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# Configuración de conexión serial con Arduino\n",
    "serial_port = 'COM5'  # Ajusta al puerto COM donde esté el Arduino\n",
    "baud_rate = 9600\n",
    "ser = serial.Serial(serial_port, baud_rate)\n",
    "time.sleep(2)  # Tiempo para que se establezca la conexión\n",
    "\n",
    "# Inicializar la cámara\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def enviar_angulo(angulo):\n",
    "    \"\"\"\n",
    "    Envía un ángulo al microcontrolador para mover el servo.\n",
    "    :param angulo: int, ángulo entre 0 y 180\n",
    "    \"\"\"\n",
    "    if 0 <= angulo <= 180:\n",
    "        ser.write(f'{angulo}\\n'.encode())\n",
    "        print(f\"Ángulo enviado: {angulo} grados\")\n",
    "    else:\n",
    "        print(\"El ángulo debe estar entre 0 y 180 grados\")\n",
    "\n",
    "def predecir_gesto(landmarks):\n",
    "    \"\"\"\n",
    "    Realiza la predicción del gesto y retorna el nombre del gesto.\n",
    "    :param landmarks: list, lista de landmarks de la mano\n",
    "    :return: str, nombre del gesto\n",
    "    \"\"\"\n",
    "    landmarks_array = np.array(landmarks).reshape(1, -1)\n",
    "    prediction = model.predict(landmarks_array)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    gesture_name = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    return gesture_name\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"No se pudo leer el marco de la cámara.\")\n",
    "            break\n",
    "\n",
    "        # Convertir la imagen a RGB para Mediapipe\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Dibujar los landmarks\n",
    "                mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Extraer los landmarks en formato de lista con 5 decimales\n",
    "                landmarks = []\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    landmarks.append(round(landmark.x, 5))\n",
    "                    landmarks.append(round(landmark.y, 5))\n",
    "\n",
    "                # Comprobar la longitud de los landmarks\n",
    "                if len(landmarks) != 42:\n",
    "                    raise ValueError(f\"Se esperaban 42 valores, pero se recibieron {len(landmarks)}.\")\n",
    "                \n",
    "                try:\n",
    "                    # Predecir el gesto\n",
    "                    gesture_name = predecir_gesto(landmarks)\n",
    "                    cv2.putText(image, f\"Gesto: {gesture_name}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                    # Enviar ángulo específico al Arduino según el gesto detectado\n",
    "                    if gesture_name == \"mano_abierta\":\n",
    "                        enviar_angulo(0)   # Ejemplo: ángulo 0\n",
    "                    elif gesture_name == \"paz\":\n",
    "                        enviar_angulo(90)  # Ejemplo: ángulo 90\n",
    "                    elif gesture_name == \"rock\":\n",
    "                        enviar_angulo(180)  # Ejemplo: ángulo 180\n",
    "\n",
    "                except Exception as e:\n",
    "                    cv2.putText(image, f\"Ocurrió un error: {e}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    print(\"Error:\", e)\n",
    "\n",
    "        else:\n",
    "            cv2.putText(image, \"Esperando detección de mano...\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "        # Mostrar la imagen con los resultados\n",
    "        cv2.imshow('Detección de Gestos', image)\n",
    "\n",
    "        # Salir con la tecla 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # Liberar la captura y cerrar conexiones\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    ser.close()\n",
    "    print(\"Conexión cerrada.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
